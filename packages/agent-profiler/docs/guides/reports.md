# Reports

Understand the report structure, output files, and how to interpret statistical results.

## Report Generation

Reports are generated by calling `generateReport(options)`, which creates a timestamped directory containing multiple Markdown pages, data exports, and per-scenario detail files.

```typescript
import { generateReport } from "@ghx-dev/agent-profiler"
import type { ReportOptions } from "@ghx-dev/agent-profiler"

const options: ReportOptions = {
  runId: "run-2026-02-27-001",
  rows: result.rows,
  reportsDir: "reports",
  analysisResults: result.analysisResults,  // optional
}

const report = await generateReport(options)
```

### ReportOptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `runId` | `string` | Yes | Unique identifier for the profiling run |
| `rows` | `ProfileRow[]` | Yes | All profile rows from the suite execution |
| `reportsDir` | `string` | Yes | Base directory for report output |
| `analysisResults` | `SessionAnalysisBundle[]` | No | Analyzer findings grouped by analyzer name |

## Output Structure

Each report generates a timestamped directory with the following layout:

```text
reports/<timestamp>/
  index.md          -- summary page
  metrics.md        -- per-metric descriptive statistics
  analysis.md       -- analyzer findings
  comparison.md     -- cross-mode comparison tables
  scenarios/
    <id>.md         -- per-scenario detail page
  data/
    results.csv     -- raw CSV export
    results.json    -- raw JSON export
    summary.json    -- aggregated summary
```

## Reading the Summary Page (index.md)

The summary page provides a high-level overview of the profiling run.

Contents include:

- **Run ID** -- unique identifier for traceability
- **Mode/scenario matrix** -- which modes and scenarios were profiled, with iteration counts
- **Overall success rates** -- percentage of iterations where the scorer returned `success: true`, broken down by mode
- **Timing overview** -- total suite duration and average iteration duration per mode
- **Configuration snapshot** -- key settings (repetitions, warmup, timeout, retries)

This page answers the question: "did the profiling run complete, and what did it cover?"

## Reading the Metrics Page (metrics.md)

The metrics page presents descriptive statistics for every numeric metric collected during the run, grouped by mode and scenario.

For each metric, the page displays:

| Statistic | Description |
|-----------|-------------|
| `mean` | Arithmetic mean across repetitions |
| `median` | 50th percentile (middle value) |
| `p90` | 90th percentile |
| `p95` | 95th percentile |
| `stddev` | Standard deviation |
| `CV` | Coefficient of variation (stddev / mean), measures relative variability |
| `IQR` | Interquartile range (p75 - p25), robust measure of spread |

Metrics include both built-in values (tokens, timing, cost, tool calls) and any custom metrics produced by registered collectors.

A low CV (below 0.1) indicates stable, reproducible measurements. A high CV (above 0.3) suggests high variability that may warrant more repetitions or investigation into what causes the variance.

## Reading the Comparison Page (comparison.md)

The comparison page presents cross-mode statistical comparisons. For each metric where two modes have measurements, the page includes:

| Column | Description |
|--------|-------------|
| `reductionPct` | Percentage reduction from mode A to mode B. Positive means mode A is better (lower value). |
| `95% CI` | Bootstrap 95% confidence interval for the reduction percentage. |
| `Cohen's d` | Standardized effect size measuring the practical difference between groups. |
| `Effect Magnitude` | Qualitative classification of Cohen's d. |
| `p-value` | Permutation test p-value for statistical significance. |

### Interpreting Comparison Results

**Reduction percentage:** A positive `reductionPct` means mode A produced lower values than mode B for that metric. For metrics where lower is better (latency, tokens, cost), positive reduction indicates mode A outperforms mode B.

**Confidence interval:** If the 95% CI does not cross zero, the reduction is statistically significant at the 5% level. A CI of `[5.2%, 18.7%]` means the true reduction is between 5.2% and 18.7% with 95% confidence.

**Effect magnitude classification:**

| Cohen's d | Magnitude | Interpretation |
|-----------|-----------|----------------|
| < 0.2 | Negligible | No practical difference between modes |
| 0.2 -- 0.5 | Small | Detectable but minor difference |
| 0.5 -- 0.8 | Medium | Meaningful practical difference |
| > 0.8 | Large | Substantial practical difference |

**p-value:** A p-value below 0.05 indicates the observed difference is unlikely due to chance alone. Combined with a non-zero-crossing CI and a medium or large effect size, this provides strong evidence of a real performance difference.

## Reading the Analysis Page (analysis.md)

The analysis page presents findings from all registered analyzers, grouped by analyzer name.

Each analyzer section contains:

- **Summary** -- the human-readable summary from the `AnalysisResult`
- **Findings** -- each named finding rendered according to its type:
  - Number findings display as `value unit` (e.g., "342 ms")
  - String findings display as prose
  - List findings display as bullet points
  - Table findings display as Markdown tables
  - Ratio findings display as percentage labels (e.g., "85% tool success rate")

The `SessionAnalysisBundle` groups results across iterations, so the analysis page shows patterns across the full dataset rather than a single iteration.

## Per-Scenario Detail Pages (scenarios/<id>.md)

Each scenario gets a dedicated detail page containing:

- Scenario metadata (name, description, tags, timeout, retries)
- Per-mode results with all metrics
- Scorer results (pass/fail, individual check details)
- Iteration-level data for identifying outliers

## Data Exports

The `data/` subdirectory contains machine-readable exports:

| File | Format | Contents |
|------|--------|----------|
| `results.csv` | CSV | Every ProfileRow field as a flat column |
| `results.json` | JSON | Full ProfileRow array with nested structures preserved |
| `summary.json` | JSON | Aggregated statistics per mode per scenario per metric |

These files enable downstream processing in spreadsheets, notebooks, or custom analysis scripts.

## Source Reference

- Report orchestrator: `packages/agent-profiler/src/reporter/orchestrator.ts`
- Statistics engine: `packages/agent-profiler/src/stats/`
- JSONL store: `packages/agent-profiler/src/store/`

## Related Documentation

- [Configuration](configuration.md) -- controlling report output paths
- [Custom Collectors](custom-collectors.md) -- custom metrics that appear in reports
- [Custom Analyzers](custom-analyzers.md) -- custom findings that appear on the analysis page
- [Quick Start](../getting-started/quick-start.md) -- generating a report from a stub run
- [Architecture Overview](../architecture/overview.md) -- reporting layer in context
